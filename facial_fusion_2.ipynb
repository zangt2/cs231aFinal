{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zgef7IRuemK-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19307dce-ad2b-4516-db50-87717c932581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "src_dir = '/content/drive/MyDrive/facial_fuse/label-distribution-learning-fer-tf'\n",
        "dst_dir = '/content/facial_fuse/label-distribution-learning-fer-tf'\n",
        "\n",
        "os.makedirs(os.path.dirname(dst_dir), exist_ok=True)\n",
        "\n",
        "shutil.copytree(src_dir, dst_dir, dirs_exist_ok=True)\n",
        "\n",
        "print(\" Folder copied to /content/facial_fuse/label-distribution-learning-fer-tf\")"
      ],
      "metadata": {
        "id": "xgRW27SbsPll",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "7d4716da-92ce-4d28-8d79-698037183025"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/facial_fuse/label-distribution-learning-fer-tf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2268530893>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs_exist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Folder copied to /content/facial_fuse/label-distribution-learning-fer-tf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \"\"\"\n\u001b[1;32m    570\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shutil.copytree\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/facial_fuse/label-distribution-learning-fer-tf'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "with open('/content/drive/MyDrive/facial_fuse/train_data.pkl', 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/facial_fuse/test_data.pkl', 'rb') as f:\n",
        "    test_data = pickle.load(f)\n",
        "\n",
        "print(f\" Loaded {len(train_data)} training samples.\")\n",
        "print(f\"Loaded {len(test_data)} testing samples.\")\n",
        "print(\"First train sample keys:\", train_data[0].keys())\n",
        "\n",
        "sample = train_data[0]\n",
        "print(\"\\nSample Structure:\")\n",
        "for k, v in sample.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "# === Optional: Check Landmark Shape ===\n",
        "import numpy as np\n",
        "landmark = np.load(sample['npy'])\n",
        "print(f\"\\nLandmark shape: {landmark.shape}\")\n",
        "\n",
        "# === Check Image Shape ===\n",
        "img = Image.open(sample['img']).convert('RGB')\n",
        "img_array = np.array(img)\n",
        "print(f\"Image shape: {img_array.shape}\")  # Expecting (H, W, 3)"
      ],
      "metadata": {
        "id": "bW02aMHyfN-m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "fca7a87b-39db-4d90-ff64-280f7c3a8c54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/facial_fuse/train_data.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-971591115>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/facial_fuse/train_data.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/facial_fuse/train_data.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def preview_samples(data_list, n=4):\n",
        "    plt.figure(figsize=(n * 3, 3))\n",
        "    for i in range(min(n, len(data_list))):\n",
        "        sample = data_list[i]\n",
        "        img = Image.open(sample['img']).convert('RGB')\n",
        "        img_array = np.array(img)\n",
        "\n",
        "        lmk = np.load(sample['npy'])\n",
        "\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.imshow(img_array)\n",
        "        if lmk.shape[1] >= 2:\n",
        "            plt.scatter(lmk[:, 0], lmk[:, 1], s=1, c='red')\n",
        "        plt.title(f\"Label: {sample['label']}\")\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "preview_samples(train_data, n=4)\n"
      ],
      "metadata": {
        "id": "sO69h4GvqZ0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "824bf8fa-6b99-40ae-ce82-34f953d5dc90"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-665347030>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mpreview_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "mlp_backbone = tf.keras.models.load_model('/content/facial_fuse/label-distribution-learning-fer-tf/pretrained/mlp_landmark_backbone.h5')\n",
        "resnet_backbone = tf.keras.models.load_model('/content/facial_fuse/label-distribution-learning-fer-tf/pretrained/resnet50.h5')\n",
        "\n",
        "resnet_backbone.trainable = True\n",
        "mlp_backbone.trainable = True\n",
        "\n",
        "class ConfidenceLateFusionModel(tf.keras.Model):\n",
        "    def __init__(self, image_backbone, lmk_backbone, num_classes=7):\n",
        "        super().__init__()\n",
        "        self.image_backbone = image_backbone\n",
        "        self.lmk_backbone = lmk_backbone\n",
        "        self.shared_fc = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.3)\n",
        "        ])\n",
        "        self.class_head = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "        self.conf_head = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        image_input, landmark_input = inputs\n",
        "        img_feat = self.image_backbone(image_input, training=training)\n",
        "        lmk_feat = self.lmk_backbone(landmark_input, training=training)\n",
        "        fused = tf.concat([img_feat, lmk_feat], axis=-1)\n",
        "        x = self.shared_fc(fused, training=training)\n",
        "        p = self.class_head(x, training=training)\n",
        "        c = self.conf_head(x, training=training)\n",
        "        return p, tf.squeeze(c, axis=-1)\n",
        "\n",
        "model = ConfidenceLateFusionModel(resnet_backbone, mlp_backbone)\n",
        "\n",
        "NUM_CLASSES = 7\n",
        "\n",
        "def preprocess_confidence_sample(sample):\n",
        "    img = Image.open(sample['img']).convert('RGB').resize((112, 112))\n",
        "    img_arr = np.array(img).astype(np.float32)\n",
        "    img_arr = (img_arr - 127.5) / 127.5\n",
        "    img_arr = np.transpose(img_arr, (2, 0, 1))\n",
        "\n",
        "    lmk = np.load(sample['npy']).astype(np.float32).flatten()\n",
        "\n",
        "    label = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
        "    label[sample['label']] = 1.0\n",
        "\n",
        "    return img_arr, lmk, label\n",
        "\n",
        "def dual_input_generator(data_list):\n",
        "    for sample in data_list:\n",
        "        img, lmk, label = preprocess_confidence_sample(sample)\n",
        "        yield (img, lmk), label\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: dual_input_generator(train_data),\n",
        "    output_signature=(\n",
        "        (tf.TensorSpec(shape=(3, 112, 112), dtype=tf.float32),\n",
        "         tf.TensorSpec(shape=(1434,), dtype=tf.float32)),\n",
        "        tf.TensorSpec(shape=(NUM_CLASSES,), dtype=tf.float32)\n",
        "    )\n",
        ")\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_dataset = train_dataset.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "def full_confidence_loss(y_true, y_pred, confidence, epoch, reg_weight=0.01, max_conf_weight=1.0):\n",
        "    kl = tf.keras.losses.KLDivergence(reduction=tf.keras.losses.Reduction.NONE)\n",
        "    per_sample_kl = kl(y_true, y_pred)\n",
        "    warmup_factor = tf.minimum(tf.exp(epoch / 2.0) / 10.0, 1.0)\n",
        "    weighted_kl = tf.reduce_mean((warmup_factor * tf.stop_gradient(confidence) + (1 - warmup_factor)) * per_sample_kl)\n",
        "    reg_term = tf.reduce_mean((1.0 - confidence) ** 2)\n",
        "    return weighted_kl + reg_weight * reg_term\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean()\n",
        "train_acc = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss.reset_state()\n",
        "    train_acc.reset_state()\n",
        "    print(f\"\\n Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    for batch in tqdm(train_dataset):\n",
        "        (img_batch, lmk_batch), label_batch = batch\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            pred_dist, conf = model((img_batch, lmk_batch), training=True)\n",
        "            loss = full_confidence_loss(label_batch, pred_dist, conf, epoch)\n",
        "\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        train_loss.update_state(loss)\n",
        "        train_acc.update_state(label_batch, pred_dist)\n",
        "\n",
        "    print(f\" Loss: {train_loss.result():.4f} |  Accuracy: {train_acc.result():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "X_H8NCZXp7hA",
        "outputId": "5b952e93-c43d-426b-94e6-6ea27a9254ac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = '/content/facial_fuse/label-distribution-learning-fer-tf/pretrained/mlp_landmark_backbone.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1122815862>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmlp_backbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/facial_fuse/label-distribution-learning-fer-tf/pretrained/mlp_landmark_backbone.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mresnet_backbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/facial_fuse/label-distribution-learning-fer-tf/pretrained/resnet50.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = '/content/facial_fuse/label-distribution-learning-fer-tf/pretrained/mlp_landmark_backbone.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "def preprocess_eval_sample(sample):\n",
        "    img = Image.open(sample['img']).convert('RGB').resize((112, 112))\n",
        "    img_arr = np.array(img).astype(np.float32)\n",
        "    img_arr = (img_arr - 127.5) / 127.5\n",
        "    img_arr = np.transpose(img_arr, (2, 0, 1))\n",
        "\n",
        "    lmk = np.load(sample['npy']).astype(np.float32).flatten()\n",
        "    return img_arr, lmk, sample['label']\n",
        "\n",
        "def eval_generator(data_list):\n",
        "    for sample in data_list:\n",
        "        img, lmk, label = preprocess_eval_sample(sample)\n",
        "        yield (img, lmk), label\n",
        "\n",
        "eval_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: eval_generator(test_data),\n",
        "    output_signature=(\n",
        "        (tf.TensorSpec(shape=(3, 112, 112), dtype=tf.float32),\n",
        "         tf.TensorSpec(shape=(1434,), dtype=tf.float32)),\n",
        "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "    )\n",
        ").batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "all_preds, all_labels, all_confs, all_features = [], [], [], []\n",
        "\n",
        "for (img_batch, lmk_batch), labels in eval_dataset:\n",
        "    pred_dist, conf = model((img_batch, lmk_batch), training=False)\n",
        "    preds = tf.argmax(pred_dist, axis=1)\n",
        "\n",
        "    all_preds.extend(preds.numpy())\n",
        "    all_labels.extend(labels.numpy())\n",
        "    all_confs.extend(conf.numpy())\n",
        "\n",
        "    img_feat = model.image_backbone(img_batch, training=False)\n",
        "    lmk_feat = model.lmk_backbone(lmk_batch, training=False)\n",
        "    fused_feat = tf.concat([img_feat, lmk_feat], axis=-1)\n",
        "    all_features.extend(fused_feat.numpy())\n",
        "\n",
        "class_names = ['Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Anger']\n",
        "all_preds_np = np.array(all_preds)\n",
        "all_labels_np = np.array(all_labels)\n",
        "\n",
        "acc = accuracy_score(all_labels_np, all_preds_np)\n",
        "conf_matrix = confusion_matrix(all_labels_np, all_preds_np)\n",
        "per_class_acc = conf_matrix.diagonal() / np.maximum(conf_matrix.sum(axis=1), 1)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Emotion')\n",
        "plt.ylabel('True Emotion')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_proj = tsne.fit_transform(np.array(all_features))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(tsne_proj[:, 0], tsne_proj[:, 1], c=all_labels_np, cmap='tab10', alpha=0.6)\n",
        "handles, _ = scatter.legend_elements()\n",
        "plt.legend(handles, class_names, title='Emotions')\n",
        "plt.title('t-SNE of Fused Features')\n",
        "plt.show()\n",
        "\n",
        "print(f\" Accuracy: {acc:.4f}\")\n",
        "print(\" Per-class Accuracy:\")\n",
        "for i, a in enumerate(per_class_acc):\n",
        "    print(f\"  {class_names[i]}: {a:.4f}\")\n",
        "\n",
        "print(\"\\n Classification Report:\")\n",
        "print(classification_report(all_labels_np, all_preds_np, target_names=class_names))\n",
        "\n",
        "print(\"\\n Confidence stats:\")\n",
        "print(f\"Min: {np.min(all_confs):.4f}, Max: {np.max(all_confs):.4f}, Mean: {np.mean(all_confs):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vyZ-a5zrKGLJ",
        "outputId": "645cf411-876f-4aac-a56c-20aef2c4ae7b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnknownError",
          "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} NameError: name 'test_data' is not defined\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 865, in get_iterator\n    return self._iterators[iterator_id]\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n\nKeyError: np.int64(0)\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 867, in get_iterator\n    iterator = iter(self._generator(*self._args.pop(iterator_id)))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"<ipython-input-5-2535687053>\", line 23, in <lambda>\n    lambda: eval_generator(test_data),\n                           ^^^^^^^^^\n\nNameError: name 'test_data' is not defined\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2535687053>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mall_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_confs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmk_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mpred_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmk_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    777\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3084\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3086\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3087\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6000\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6001\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6002\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnknownError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} NameError: name 'test_data' is not defined\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 865, in get_iterator\n    return self._iterators[iterator_id]\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n\nKeyError: np.int64(0)\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 867, in get_iterator\n    iterator = iter(self._generator(*self._args.pop(iterator_id)))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"<ipython-input-5-2535687053>\", line 23, in <lambda>\n    lambda: eval_generator(test_data),\n                           ^^^^^^^^^\n\nNameError: name 'test_data' is not defined\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.save_weights('/content/confidence_latefusion.weights.h5')\n",
        "print(\"Model weights saved to 'confidence_latefusion.weights.h5'\")\n",
        "\n",
        "\n",
        "restored_model = ConfidenceLateFusionModel(resnet_backbone, mlp_backbone)\n",
        "\n",
        "dummy_img = tf.random.normal((1, 3, 112, 112))\n",
        "dummy_lmk = tf.random.normal((1, 1434))\n",
        "restored_model((dummy_img, dummy_lmk), training=False)\n",
        "\n",
        "restored_model.load_weights('/content/confidence_latefusion.weights.h5')\n",
        "print(\"Weights successfully reloaded into restored_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "vUj5SnVzkc9X",
        "outputId": "e0d2891d-9b56-4a97-d167-3c48fc1de2b5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-194473558>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/confidence_latefusion.weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model weights saved to 'confidence_latefusion.weights.h5'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrestored_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfidenceLateFusionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet_backbone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp_backbone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# model.save(\"conf_head.h5\")\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download(\"conf_head.h5\")"
      ],
      "metadata": {
        "id": "QHlQqmBhUVFw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install tensorflow-addons\n"
      ],
      "metadata": {
        "id": "TnnhPAlqNSP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SjC5LdH_jZkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "def pairwise_distance(embeddings, squared=False):\n",
        "    dot = tf.matmul(embeddings, tf.transpose(embeddings))\n",
        "    square_norm = tf.linalg.diag_part(dot)\n",
        "    distances = tf.expand_dims(square_norm, 0) - 2.0 * dot + tf.expand_dims(square_norm, 1)\n",
        "    distances = tf.maximum(distances, 0.0)\n",
        "    if not squared:\n",
        "        mask = tf.cast(tf.equal(distances, 0.0), tf.float32)\n",
        "        distances = distances + mask * 1e-16\n",
        "        distances = tf.sqrt(distances)\n",
        "        distances = distances * (1.0 - mask)\n",
        "    return distances\n",
        "\n",
        "def custom_triplet_semihard_loss(labels, embeddings, margin=0.2):\n",
        "    labels = tf.reshape(labels, [-1, 1])\n",
        "    pdist_matrix = pairwise_distance(embeddings)\n",
        "\n",
        "    adjacency = tf.equal(labels, tf.transpose(labels))\n",
        "    adjacency_not = tf.logical_not(adjacency)\n",
        "\n",
        "    def compute_loss(i):\n",
        "        dist_i = pdist_matrix[i]\n",
        "        label_i = labels[i]\n",
        "\n",
        "        pos_mask = adjacency[i]\n",
        "        neg_mask = adjacency_not[i]\n",
        "\n",
        "        pos_dists = tf.boolean_mask(dist_i, pos_mask)\n",
        "        neg_dists = tf.boolean_mask(dist_i, neg_mask)\n",
        "\n",
        "        def per_pos_loss(pos_dist):\n",
        "            semi_hard_negatives = tf.boolean_mask(neg_dists, neg_dists > pos_dist)\n",
        "            semi_hard_negatives = tf.boolean_mask(semi_hard_negatives, semi_hard_negatives < pos_dist + margin)\n",
        "            if tf.size(semi_hard_negatives) > 0:\n",
        "                neg_dist = tf.reduce_min(semi_hard_negatives)\n",
        "                return tf.maximum(pos_dist - neg_dist + margin, 0.0)\n",
        "            else:\n",
        "                return 0.0\n",
        "\n",
        "        losses = tf.map_fn(per_pos_loss, pos_dists, dtype=tf.float32)\n",
        "        return tf.reduce_mean(losses) if tf.size(losses) > 0 else 0.0\n",
        "\n",
        "    losses = tf.map_fn(compute_loss, tf.range(tf.shape(embeddings)[0]), dtype=tf.float32)\n",
        "    return tf.reduce_mean(losses)\n",
        "\n",
        "mlp_backbone = tf.keras.models.load_model('/content/facial_fuse/label-distribution-learning-fer-tf/pretrained/mlp_landmark_backbone.h5')\n",
        "resnet_backbone = tf.keras.models.load_model('/content/facial_fuse/label-distribution-learning-fer-tf/pretrained/resnet50.h5')\n",
        "\n",
        "resnet_backbone.trainable = True\n",
        "mlp_backbone.trainable = True\n",
        "\n",
        "class FullModel(tf.keras.Model):\n",
        "    def __init__(self, image_backbone, lmk_backbone, num_classes=7):\n",
        "        super().__init__()\n",
        "        self.image_backbone = image_backbone\n",
        "        self.lmk_backbone = lmk_backbone\n",
        "        self.shared_fc = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.3)\n",
        "        ])\n",
        "        self.class_head = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "        self.conf_head = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs, training=False, return_embedding=False):\n",
        "        image_input, landmark_input = inputs\n",
        "        img_feat = self.image_backbone(image_input, training=training)\n",
        "        lmk_feat = self.lmk_backbone(landmark_input, training=training)\n",
        "        fused = tf.concat([img_feat, lmk_feat], axis=-1)\n",
        "        x = self.shared_fc(fused, training=training)\n",
        "        if return_embedding:\n",
        "            return tf.math.l2_normalize(x, axis=-1)\n",
        "        p = self.class_head(x, training=training)\n",
        "        c = self.conf_head(x, training=training)\n",
        "        return p, tf.squeeze(c, axis=-1)\n",
        "\n",
        "model = FullModel(resnet_backbone, mlp_backbone)\n",
        "\n",
        "NUM_CLASSES = 7\n",
        "\n",
        "def preprocess_sample(sample):\n",
        "    img = Image.open(sample['img']).convert('RGB').resize((112, 112))\n",
        "    img = (np.array(img).astype(np.float32) - 127.5) / 127.5\n",
        "    img = np.transpose(img, (2, 0, 1))\n",
        "    lmk = np.load(sample['npy']).astype(np.float32).flatten()\n",
        "    label = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
        "    label[sample['label']] = 1.0\n",
        "    return img, lmk, label\n",
        "\n",
        "def dual_input_generator(data_list):\n",
        "    for sample in data_list:\n",
        "        img, lmk, label = preprocess_sample(sample)\n",
        "        yield (img, lmk), label\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: dual_input_generator(train_data),\n",
        "    output_signature=(\n",
        "        (tf.TensorSpec(shape=(3, 112, 112), dtype=tf.float32),\n",
        "         tf.TensorSpec(shape=(1434,), dtype=tf.float32)),\n",
        "        tf.TensorSpec(shape=(NUM_CLASSES,), dtype=tf.float32)\n",
        "    )\n",
        ").shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "kl_loss_fn = tf.keras.losses.KLDivergence(reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "def confidence_weighted_kl(y_true, y_pred, confidence, epoch, reg_weight=0.01):\n",
        "    warmup = tf.minimum(tf.exp(epoch / 2.0) / 10.0, 1.0)\n",
        "    per_sample_kl = kl_loss_fn(y_true, y_pred)\n",
        "    weighted = (warmup * tf.stop_gradient(confidence) + (1 - warmup)) * per_sample_kl\n",
        "    kl_term = tf.reduce_mean(weighted)\n",
        "    reg_term = tf.reduce_mean((1.0 - confidence) ** 2)\n",
        "    return kl_term + reg_weight * reg_term, kl_term\n",
        "\n",
        "def train_model(dataset, epochs=10, base_triplet_lambda=0.1):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "    print(\"Batch sample structure verified.\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        acc_metric.reset_state()\n",
        "        print(f\"\\n Epoch {epoch+1}/{epochs}\")\n",
        "        total_loss = 0\n",
        "        step = 0\n",
        "\n",
        "        triplet_lambda = min(epoch / 5.0, 1.0) * base_triplet_lambda\n",
        "\n",
        "        for batch in tqdm(dataset):\n",
        "            (img_batch, lmk_batch), label_batch = batch\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                pred_dist, conf = model((img_batch, lmk_batch), training=True)\n",
        "                embeddings = model((img_batch, lmk_batch), training=True, return_embedding=True)\n",
        "\n",
        "                hard_labels = tf.argmax(label_batch, axis=1)\n",
        "                triplet_loss = custom_triplet_semihard_loss(hard_labels, embeddings)\n",
        "\n",
        "                total_kl, kl_only = confidence_weighted_kl(label_batch, pred_dist, conf, epoch)\n",
        "                loss = total_kl + triplet_lambda * triplet_loss\n",
        "\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "            total_loss += loss.numpy()\n",
        "            acc_metric.update_state(label_batch, pred_dist)\n",
        "\n",
        "            if step % 20 == 0:\n",
        "                print(f\" Step {step} | KL: {kl_only.numpy():.4f} | Triplet: {triplet_loss.numpy():.4f} | Conf µ: {tf.reduce_mean(conf).numpy():.3f}\")\n",
        "            step += 1\n",
        "\n",
        "        print(f\"📉 Loss: {total_loss/step:.4f} | 🎯 Accuracy: {acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "train_model(train_dataset, epochs=10, base_triplet_lambda=0.1)\n"
      ],
      "metadata": {
        "id": "JpI2EJ0osAU6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "outputId": "f9274766-fa56-49b9-f975-0fa17e0948ed"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = '/content/facial_fuse/label-distribution-learning-fer-tf/pretrained/mlp_landmark_backbone.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-318471366>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mmlp_backbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/facial_fuse/label-distribution-learning-fer-tf/pretrained/mlp_landmark_backbone.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mresnet_backbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/facial_fuse/label-distribution-learning-fer-tf/pretrained/resnet50.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = '/content/facial_fuse/label-distribution-learning-fer-tf/pretrained/mlp_landmark_backbone.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.manifold import TSNE\n",
        "from tqdm import tqdm\n",
        "\n",
        "class_names = ['Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Anger']\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: dual_input_generator(test_data),\n",
        "    output_signature=(\n",
        "        (tf.TensorSpec(shape=(3, 112, 112), dtype=tf.float32),\n",
        "         tf.TensorSpec(shape=(1434,), dtype=tf.float32)),\n",
        "        tf.TensorSpec(shape=(NUM_CLASSES,), dtype=tf.float32)\n",
        "    )\n",
        ").batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "all_embeddings = []\n",
        "acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "\n",
        "print(\" Running Evaluation...\")\n",
        "for (img_batch, lmk_batch), label_batch in tqdm(test_dataset):\n",
        "    pred_dist, _ = model((img_batch, lmk_batch), training=False)\n",
        "    embeddings = model((img_batch, lmk_batch), training=False, return_embedding=True)\n",
        "\n",
        "    all_preds.extend(np.argmax(pred_dist.numpy(), axis=1))\n",
        "    all_labels.extend(np.argmax(label_batch.numpy(), axis=1))\n",
        "    all_embeddings.append(embeddings.numpy())\n",
        "    acc_metric.update_state(label_batch, pred_dist)\n",
        "\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "all_embeddings = np.vstack(all_embeddings)\n",
        "\n",
        "print(\" Classification Report:\\n\")\n",
        "print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))\n",
        "\n",
        "conf_mat = confusion_matrix(all_labels, all_preds)\n",
        "per_class_acc = conf_mat.diagonal() / conf_mat.sum(axis=1)\n",
        "print(\"\\n Per-Class Accuracy (Recall):\")\n",
        "for i, acc in enumerate(per_class_acc):\n",
        "    print(f\"{class_names[i]:<10}: {acc:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix Heatmap')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"🔍 Computing t-SNE Embedding...\")\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_proj = tsne.fit_transform(all_embeddings)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(tsne_proj[:, 0], tsne_proj[:, 1], c=all_labels, cmap='tab10', alpha=0.6)\n",
        "handles, _ = scatter.legend_elements()\n",
        "plt.legend(handles, class_names, title='Emotions', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.title('t-SNE of Fused Embeddings')\n",
        "plt.xlabel('t-SNE Dimension 1')\n",
        "plt.ylabel('t-SNE Dimension 2')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n Final Test Accuracy: {acc_metric.result().numpy():.4f}\")\n"
      ],
      "metadata": {
        "id": "hbVkBwBtt0qr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "77f44058-941c-4085-acdf-6de75c679620"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'NUM_CLASSES' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-2575365150>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         (tf.TensorSpec(shape=(3, 112, 112), dtype=tf.float32),\n\u001b[1;32m     15\u001b[0m          tf.TensorSpec(shape=(1434,), dtype=tf.float32)),\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     18\u001b[0m ).batch(32).prefetch(tf.data.AUTOTUNE)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'NUM_CLASSES' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_model(train_dataset, epochs=10, lambda_triplet=0.1)\n"
      ],
      "metadata": {
        "id": "2Ly5O7OwNEcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.save_weights(\"full_model.weights.h5\")\n",
        "print(\"Model saved as 'full_model_weights.h5'\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"full_model.weights.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "ehqyMZktUoft",
        "outputId": "d27f9e2c-17b6-4a16-c696-52fca32ca1b3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3004455753>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"full_model.weights.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model saved as 'full_model_weights.h5'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"full_model.weights.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vVS7_kYPKHSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "def pairwise_distance(embeddings, squared=False):\n",
        "    dot = tf.matmul(embeddings, tf.transpose(embeddings))\n",
        "    square_norm = tf.linalg.diag_part(dot)\n",
        "    distances = tf.expand_dims(square_norm, 0) - 2.0 * dot + tf.expand_dims(square_norm, 1)\n",
        "    distances = tf.maximum(distances, 0.0)\n",
        "    if not squared:\n",
        "        mask = tf.cast(tf.equal(distances, 0.0), tf.float32)\n",
        "        distances = distances + mask * 1e-16\n",
        "        distances = tf.sqrt(distances)\n",
        "        distances = distances * (1.0 - mask)\n",
        "    return distances\n",
        "\n",
        "def custom_triplet_semihard_loss(labels, embeddings, margin=0.2):\n",
        "    labels = tf.reshape(labels, [-1, 1])\n",
        "    pdist_matrix = pairwise_distance(embeddings)\n",
        "\n",
        "    adjacency = tf.equal(labels, tf.transpose(labels))\n",
        "    adjacency_not = tf.logical_not(adjacency)\n",
        "\n",
        "    def compute_loss(i):\n",
        "        dist_i = pdist_matrix[i]\n",
        "        label_i = labels[i]\n",
        "\n",
        "        pos_mask = adjacency[i]\n",
        "        neg_mask = adjacency_not[i]\n",
        "\n",
        "        pos_dists = tf.boolean_mask(dist_i, pos_mask)\n",
        "        neg_dists = tf.boolean_mask(dist_i, neg_mask)\n",
        "\n",
        "        def per_pos_loss(pos_dist):\n",
        "            semi_hard_negatives = tf.boolean_mask(neg_dists, neg_dists > pos_dist)\n",
        "            semi_hard_negatives = tf.boolean_mask(semi_hard_negatives, semi_hard_negatives < pos_dist + margin)\n",
        "            if tf.size(semi_hard_negatives) > 0:\n",
        "                neg_dist = tf.reduce_min(semi_hard_negatives)\n",
        "                return tf.maximum(pos_dist - neg_dist + margin, 0.0)\n",
        "            else:\n",
        "                return 0.0\n",
        "\n",
        "        losses = tf.map_fn(per_pos_loss, pos_dists, dtype=tf.float32)\n",
        "        return tf.reduce_mean(losses) if tf.size(losses) > 0 else 0.0\n",
        "\n",
        "    losses = tf.map_fn(compute_loss, tf.range(tf.shape(embeddings)[0]), dtype=tf.float32)\n",
        "    return tf.reduce_mean(losses)\n",
        "\n",
        "\n",
        "mlp_backbone = tf.keras.models.load_model('/content/facial_fuse/label-distribution-learning-fer-tf/pretrained/mlp_landmark_backbone.h5')\n",
        "resnet_backbone = tf.keras.models.load_model('/content/facial_fuse/label-distribution-learning-fer-tf/pretrained/resnet50.h5')\n",
        "\n",
        "resnet_backbone.trainable = True\n",
        "mlp_backbone.trainable = True\n",
        "\n",
        "class ContrastiveLossModel(tf.keras.Model):\n",
        "    def __init__(self, image_backbone, lmk_backbone, num_classes=7):\n",
        "        super().__init__()\n",
        "        self.image_backbone = image_backbone\n",
        "        self.lmk_backbone = lmk_backbone\n",
        "        self.shared_fc = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.3)\n",
        "        ])\n",
        "        self.class_head = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training=False, return_embedding=False):\n",
        "        image_input, landmark_input = inputs\n",
        "        img_feat = self.image_backbone(image_input, training=training)\n",
        "        lmk_feat = self.lmk_backbone(landmark_input, training=training)\n",
        "        fused = tf.concat([img_feat, lmk_feat], axis=-1)\n",
        "        x = self.shared_fc(fused, training=training)\n",
        "        if return_embedding:\n",
        "            return tf.math.l2_normalize(x, axis=-1)\n",
        "        return self.class_head(x, training=training)\n",
        "\n",
        "model = ContrastiveLossModel(resnet_backbone, mlp_backbone)\n",
        "\n",
        "NUM_CLASSES = 7\n",
        "\n",
        "def preprocess_sample(sample):\n",
        "    img = Image.open(sample['img']).convert('RGB').resize((112, 112))\n",
        "    img = (np.array(img).astype(np.float32) - 127.5) / 127.5\n",
        "    img = np.transpose(img, (2, 0, 1))\n",
        "    lmk = np.load(sample['npy']).astype(np.float32).flatten()\n",
        "    label = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
        "    label[sample['label']] = 1.0\n",
        "    return img, lmk, label\n",
        "\n",
        "def dual_input_generator(data_list):\n",
        "    for sample in data_list:\n",
        "        img, lmk, label = preprocess_sample(sample)\n",
        "        yield (img, lmk), label\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: dual_input_generator(train_data),\n",
        "    output_signature=(\n",
        "        (tf.TensorSpec(shape=(3, 112, 112), dtype=tf.float32),\n",
        "         tf.TensorSpec(shape=(1434,), dtype=tf.float32)),\n",
        "        tf.TensorSpec(shape=(NUM_CLASSES,), dtype=tf.float32)\n",
        "    )\n",
        ").shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "kl_loss_fn = tf.keras.losses.KLDivergence(reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "def weighted_kl(y_true, y_pred, epoch, reg_weight=0.01):\n",
        "    warmup = tf.minimum(tf.exp(epoch / 2.0) / 10.0, 1.0)\n",
        "    per_sample_kl = kl_loss_fn(y_true, y_pred)\n",
        "    weighted = warmup * per_sample_kl\n",
        "    kl_term = tf.reduce_mean(weighted)\n",
        "    return kl_term, kl_term\n",
        "\n",
        "def train_model(dataset, epochs=10, base_triplet_lambda=0.1):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "    print(\" Batch sample structure verified.\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        acc_metric.reset_state()\n",
        "        print(f\"\\n Epoch {epoch+1}/{epochs}\")\n",
        "        total_loss = 0\n",
        "        step = 0\n",
        "\n",
        "        triplet_lambda = min(epoch / 5.0, 1.0) * base_triplet_lambda\n",
        "\n",
        "        for batch in tqdm(dataset):\n",
        "            (img_batch, lmk_batch), label_batch = batch\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                pred_dist = model((img_batch, lmk_batch), training=True)\n",
        "                embeddings = model((img_batch, lmk_batch), training=True, return_embedding=True)\n",
        "\n",
        "                hard_labels = tf.argmax(label_batch, axis=1)\n",
        "                triplet_loss = custom_triplet_semihard_loss(hard_labels, embeddings)\n",
        "\n",
        "                total_kl, kl_only = weighted_kl(label_batch, pred_dist, epoch)\n",
        "                loss = total_kl + triplet_lambda * triplet_loss\n",
        "\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "            total_loss += loss.numpy()\n",
        "            acc_metric.update_state(label_batch, pred_dist)\n",
        "\n",
        "            if step % 20 == 0:\n",
        "                print(f\" Step {step} | KL: {kl_only.numpy():.4f} | Triplet: {triplet_loss.numpy():.4f}\")\n",
        "            step += 1\n",
        "\n",
        "        print(f\"📉 Loss: {total_loss/step:.4f} | 🎯 Accuracy: {acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "train_model(train_dataset, epochs=10, base_triplet_lambda=0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "2RYeunX4EBBG",
        "outputId": "f3b57a8c-2250-4e10-e2b1-51f3b679bf7d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = '/content/facial_fuse/label-distribution-learning-fer-tf/pretrained/mlp_landmark_backbone.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-3872870802>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mmlp_backbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/facial_fuse/label-distribution-learning-fer-tf/pretrained/mlp_landmark_backbone.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mresnet_backbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/facial_fuse/label-distribution-learning-fer-tf/pretrained/resnet50.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = '/content/facial_fuse/label-distribution-learning-fer-tf/pretrained/mlp_landmark_backbone.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.manifold import TSNE\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class_names = ['Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Anger']\n",
        "NUM_CLASSES = len(class_names)\n",
        "\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: dual_input_generator(test_data),\n",
        "    output_signature=(\n",
        "        (tf.TensorSpec(shape=(3, 112, 112), dtype=tf.float32),\n",
        "         tf.TensorSpec(shape=(1434,), dtype=tf.float32)),\n",
        "        tf.TensorSpec(shape=(NUM_CLASSES,), dtype=tf.float32)\n",
        "    )\n",
        ").batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "all_embeddings = []\n",
        "acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "print(\"Running Evaluation...\")\n",
        "for (img_batch, lmk_batch), label_batch in tqdm(test_dataset):\n",
        "    pred_dist = model((img_batch, lmk_batch), training=False)\n",
        "    embeddings = model((img_batch, lmk_batch), training=False, return_embedding=True)\n",
        "\n",
        "    all_preds.extend(np.argmax(pred_dist.numpy(), axis=1))\n",
        "    all_labels.extend(np.argmax(label_batch.numpy(), axis=1))\n",
        "    all_embeddings.append(embeddings.numpy())\n",
        "    acc_metric.update_state(label_batch, pred_dist)\n",
        "\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "all_embeddings = np.vstack(all_embeddings)\n",
        "\n",
        "print(\" Classification Report:\\n\")\n",
        "print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))\n",
        "\n",
        "conf_mat = confusion_matrix(all_labels, all_preds)\n",
        "per_class_acc = conf_mat.diagonal() / np.maximum(conf_mat.sum(axis=1), 1)\n",
        "print(\"\\n Per-Class Accuracy (Recall):\")\n",
        "for i, acc in enumerate(per_class_acc):\n",
        "    print(f\"{class_names[i]:<10}: {acc:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix Heatmap')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\" Computing t-SNE Embedding...\")\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_proj = tsne.fit_transform(all_embeddings)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(tsne_proj[:, 0], tsne_proj[:, 1], c=all_labels, cmap='tab10', alpha=0.6)\n",
        "handles, _ = scatter.legend_elements()\n",
        "plt.legend(handles, class_names, title='Emotions', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.title('t-SNE of Fused Embeddings')\n",
        "plt.xlabel('t-SNE Dimension 1')\n",
        "plt.ylabel('t-SNE Dimension 2')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n Final Test Accuracy (Top-1): {acc_metric.result().numpy():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1QEz6eHkbdcG",
        "outputId": "5afb32b5-e967-4d86-d07a-e3627f534234"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnknownError",
          "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} NameError: name 'dual_input_generator' is not defined\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 865, in get_iterator\n    return self._iterators[iterator_id]\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n\nKeyError: np.int64(0)\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 867, in get_iterator\n    iterator = iter(self._generator(*self._args.pop(iterator_id)))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"<ipython-input-12-384913227>\", line 15, in <lambda>\n    lambda: dual_input_generator(test_data),\n           ^^^^^^^^^^^^^^^^^^^^\n\nNameError: name 'dual_input_generator' is not defined\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-384913227>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running Evaluation...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmk_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mpred_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmk_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmk_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_embedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    777\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3084\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3086\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3087\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6000\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6001\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6002\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnknownError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} NameError: name 'dual_input_generator' is not defined\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 865, in get_iterator\n    return self._iterators[iterator_id]\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n\nKeyError: np.int64(0)\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 867, in get_iterator\n    iterator = iter(self._generator(*self._args.pop(iterator_id)))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"<ipython-input-12-384913227>\", line 15, in <lambda>\n    lambda: dual_input_generator(test_data),\n           ^^^^^^^^^^^^^^^^^^^^\n\nNameError: name 'dual_input_generator' is not defined\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.save_weights(\"contra_loss_only.weights.h5\")\n",
        "print(\" Model saved as 'contra_loss_only.weights.h5'\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"contra_loss_only.weights.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "H6cmjkrFgRM-",
        "outputId": "5a1572ab-9cb5-4311-ce06-fca6af770a09"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-416756931>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"contra_loss_only.weights.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Model saved as 'contra_loss_only.weights.h5'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"contra_loss_only.weights.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    }
  ]
}